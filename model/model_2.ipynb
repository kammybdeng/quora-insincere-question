{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.layers import Dense, Input, LSTM, CuDNNLSTM, Embedding, CuDNNGRU, MaxPool2D, Conv2D, Concatenate, SpatialDropout1D\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.layers import Reshape, Flatten, Dropout, Activation\n",
    "from keras.layers import Activation, Wrapper\n",
    "from keras.engine.topology import Layer\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_original shape :  (1306122, 3)\n",
      "test_original shape :  (375806, 2)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "print(\"train_original shape : \",train.shape)\n",
    "print(\"test_original shape : \",test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load and create embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_word_embedding(filepath):\n",
    "    \"\"\"\n",
    "    Given a filepath to embeddings file, return a word to vec dictionary, \n",
    "    in other words, word_embedding\n",
    "    Ex. {'word': array([1.97, -0.63, ..., 0.573, 2.54])}\n",
    "    \"\"\"\n",
    "    def _get_vec(word, *arr):\n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "    print('load word embedding ......')\n",
    "    \n",
    "    try:\n",
    "        word_embedding = dict(_get_vec(*w.split(' ')) for w in open(filepath))\n",
    "    except UnicodeDecodeError:\n",
    "        word_embedding = dict(_get_vec(*w.split(' ')) for w in open(\n",
    "            filepath, encoding=\"utf8\", errors='ignore'))\n",
    "    \n",
    "    # sanity check for word vectors must with lengths of 300\n",
    "    words_to_del = []\n",
    "    for word, vec in word_embedding.items():\n",
    "        if len(vec) != 300:\n",
    "            words_to_del.append(word)\n",
    "    for word in words_to_del:\n",
    "        del word_embedding[word]\n",
    "    return word_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load word embedding ......\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_FILE_glove = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "embeddings_index_glove = load_word_embedding(EMBEDDING_FILE_glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_punct = list(string.punctuation)\n",
    "extra_punct = [\n",
    "        ',', '.', '\"', ':', ')', '(', '!', '?', '|', ';', \"'\", '$', '&',\n",
    "        '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£',\n",
    "        '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',\n",
    "        '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', '“', '★', '”',\n",
    "        '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾',\n",
    "        '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '▒', '：', '¼', '⊕', '▼',\n",
    "        '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲',\n",
    "        'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '∙', '）', '↓', '、', '│', '（', '»',\n",
    "        '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø',\n",
    "        '¹', '≤', '‡', '√', '«', '»', '´', 'º', '¾', '¡', '§', '£', '₤']\n",
    "\n",
    "all_punct = list(set(regular_punct + extra_punct))\n",
    "\n",
    "def spacing_punctuation(text):\n",
    "    \"\"\"\n",
    "    add space before and after punctuation and symbols\n",
    "    \"\"\"\n",
    "    for punc in all_punct:\n",
    "        if punc in text:            \n",
    "            text = text.replace(punc, f\" {punc} \")\n",
    "            \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_misspell(text):\n",
    "    \"\"\"\n",
    "    misspell list (quora vs. glove)\n",
    "    \"\"\"\n",
    "    \n",
    "    mispell_dict = {'demonitisation': 'demonetization', 'demonitization': 'demonetization', \n",
    "                    'demonetisation': 'demonetization',\n",
    "                    'pokémon': 'pokemon',\n",
    "                    'Wjy': 'Why',\n",
    "                    'Whst' : 'What',\n",
    "                    'BNBR': 'Be Nice Be Respectful',\n",
    "                    'Bolsonaro': 'Jair Bolsonaro',\n",
    "                    'XXXTentacion': 'Tentacion',\n",
    "                    'Žižek': 'Slovenian philosopher Slavoj Žižek',\n",
    "                    'Adityanath': 'Indian monk Yogi Adityanath',\n",
    "                    'Brexit': 'British Exit',\n",
    "                    'Brexiter': 'British Exit supporter',\n",
    "                    'Brexiters': 'British Exit supporters',\n",
    "                    'Brexiteer': 'British Exit supporter',\n",
    "                    'Brexiteers': 'British Exit supporters',\n",
    "                    'Brexiting': 'British Exit',\n",
    "                    'Brexitosis': 'British Exit disorder',\n",
    "                    'brexit': 'British Exit',\n",
    "                    'brexiters': 'British Exit supporters',\n",
    "                    'cryptocurrencies': 'cryptocurrency',\n",
    "                    'Cryptocurrency': 'cryptocurrency',\n",
    "                    'Litecoin' : 'cryptocurrency',\n",
    "                    'litecoin' : 'cryptocurrency',\n",
    "                    'altcoin' : 'cryptocurrency',\n",
    "                    'altcoins' : 'cryptocurrency',\n",
    "                    'jallikattu': 'Jallikattu',\n",
    "                    'Swachh': 'Swachh Bharat mission campaign ',\n",
    "                    'SJWs': 'social justice warrior',\n",
    "                    'Quorans': 'Quoran',\n",
    "                    'Qoura': 'Quora',\n",
    "                    'quoras': 'Quora',\n",
    "                    'Quroa': 'Quora',\n",
    "                    'QUORA': 'Quora',\n",
    "                    'Qoura': 'Quora',\n",
    "                    'narcissit': 'narcissist',\n",
    "                    'ethereum': 'Ethereum',\n",
    "                    'Blockchain': 'blockchain',\n",
    "                    'UCEED': 'Undergraduate Common Entrance Examination for Design',\n",
    "                    'GDPR': 'General Data Protection Regulation',\n",
    "                    'Redmi' : 'Xiaomi smartphone',\n",
    "                    'OnePlus': 'Android smartphone',\n",
    "                    'Machedo' : 'hot guy',\n",
    "                    'Coinbase':'bitcoin broker',\n",
    "                    'coinbase':'bitcoin broker',\n",
    "                    'DCEU' : 'American media franchise',\n",
    "                    'IIEST': 'Indian Institutes of Engineering Science and Technology',\n",
    "                    'Upwork' : 'global freelancing platform',\n",
    "                    'upwork' : 'global freelancing platform',\n",
    "                    'HackerRank' : 'technology company focuses on competitive programming challenges',\n",
    "                    'pokémon': 'pokemon'}\n",
    "\n",
    "    misspell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "\n",
    "    def _replace(match):\n",
    "        \"\"\"\n",
    "        reference: https://www.kaggle.com/hengzheng/attention-capsule-why-not-both-lb-0-694 # noqa\n",
    "        \"\"\"\n",
    "        try:\n",
    "            word = mispell_dict.get(match.group(0))\n",
    "        except KeyError:\n",
    "            word = match.group(0)\n",
    "            print('!!Error: Could Not Find Key: {}'.format(word))\n",
    "        return word\n",
    "    return misspell_re.sub(_replace, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    preprocess text main steps\n",
    "\n",
    "    \"\"\"\n",
    "    text = spacing_punctuation(text)\n",
    "    text = clean_misspell(text)\n",
    "   \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## some config values \n",
    "embed_size = 300 # how big is each word vector\n",
    "max_features = 150000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 75 # max number of words in a question to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1306122/1306122 [00:27<00:00, 47162.78it/s]\n",
      "100%|██████████| 375806/375806 [00:07<00:00, 50184.97it/s]\n"
     ]
    }
   ],
   "source": [
    "## preprocess\n",
    "train['question_text'] = train['question_text'].progress_apply(preprocess)\n",
    "test['question_text'] = test['question_text'].progress_apply(preprocess)\n",
    "\n",
    "## split to train and val\n",
    "train_df, val_df = train_test_split(train, test_size=0.05, random_state=42)\n",
    "\n",
    "## fill up the missing values\n",
    "train_X = train_df[\"question_text\"].fillna(\"_##_\").values\n",
    "val_X = val_df[\"question_text\"].fillna(\"_##_\").values\n",
    "test_X = test[\"question_text\"].fillna(\"_##_\").values\n",
    "\n",
    "## Tokenize the sentences\n",
    "tokenizer = Tokenizer(num_words=max_features) # glove\n",
    "\n",
    "tokenizer.fit_on_texts(list(train_X))\n",
    "train_X = tokenizer.texts_to_sequences(train_X)\n",
    "val_X = tokenizer.texts_to_sequences(val_X)\n",
    "test_X = tokenizer.texts_to_sequences(test_X)\n",
    "\n",
    "## Pad the sentences \n",
    "train_X = pad_sequences(train_X, maxlen=maxlen)\n",
    "val_X = pad_sequences(val_X, maxlen=maxlen)\n",
    "test_X = pad_sequences(test_X, maxlen=maxlen)\n",
    "\n",
    "## Get the target values\n",
    "train_y = train_df['target'].values\n",
    "val_y = val_df['target'].values  \n",
    "\n",
    "#shuffling the data\n",
    "np.random.seed(42)\n",
    "trn_idx = np.random.permutation(len(train_X))\n",
    "val_idx = np.random.permutation(len(val_X))\n",
    "\n",
    "train_X = train_X[trn_idx]\n",
    "val_X = val_X[val_idx]\n",
    "train_y = train_y[trn_idx]\n",
    "val_y = val_y[val_idx] \n",
    "\n",
    "## word index dictionary from input text/document\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_weights(word_index, embeddings_index, max_features):\n",
    "    '''\n",
    "    input: word_index from training set and embedding from embedding file\n",
    "    output: matrix with matched embedding arrays\n",
    "    '''\n",
    "    \n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    \n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    \n",
    "    # matching the word and updating the corresponding embedding vector  \n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: \n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:7: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = create_embedding_weights(word_index, embeddings_index_glove, max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_model(embedding_matrix):\n",
    "    \n",
    "    input_layer = Input(shape=(maxlen,))\n",
    "    # create embedding layer\n",
    "    x = Embedding(max_features, embed_size, weights = [embedding_matrix])(input_layer)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    # bidirectional lstm\n",
    "    x = Bidirectional(CuDNNLSTM(64, return_sequences=True), name='bidirectional_lstm')(x)\n",
    "    # bidirectional gru\n",
    "    x = Bidirectional(CuDNNGRU(64, return_sequences=True), name='bidirectional_gru')(x)\n",
    "    # global_max_pooling1d\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    x = Dense(32, activation=\"relu\", name = 'dense_1')(x)\n",
    "    x = Dense(16, activation=\"relu\", name = 'dense_2')(x)\n",
    "    # output layer\n",
    "    output_layer = Dense(1, activation=\"sigmoid\", name = 'output')(x)\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    # compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/strideradu/word2vec-and-gensim-go-go-go\n",
    "\n",
    "def train_pred(model, epochs=2):\n",
    "    for e in range(epochs):\n",
    "        model.fit(train_X, train_y, batch_size=512, epochs=1, validation_data=(val_X, val_y))\n",
    "        pred_val_y = model.predict([val_X], batch_size=1024, verbose=0)\n",
    "    pred_test_y = model.predict([test_X], batch_size=1024, verbose=0)\n",
    "    \n",
    "    \n",
    "    # Plot training & validation accuracy values\n",
    "#     plt.plot(model.history['acc'])\n",
    "#     plt.plot(model.history['val_acc'])\n",
    "#     plt.title('Model accuracy')\n",
    "#     plt.ylabel('Accuracy')\n",
    "#     plt.xlabel('Epoch')\n",
    "#     plt.legend(['Train', 'Test'], loc='upper left')\n",
    "#     plt.show()\n",
    "\n",
    "#     # Plot training & validation loss values\n",
    "#     plt.plot(model.history.history['loss'])\n",
    "#     plt.plot(model.history.history['val_loss'])\n",
    "#     plt.title('Model loss')\n",
    "#     plt.ylabel('Loss')\n",
    "#     plt.xlabel('Epoch')\n",
    "#     plt.legend(['Train', 'Test'], loc='upper left')\n",
    "#     plt.show()\n",
    "    return pred_val_y, pred_test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model = lstm_model(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 1240815 samples, validate on 65307 samples\n",
      "Epoch 1/1\n",
      "1240815/1240815 [==============================] - 148s 120us/step - loss: 0.1116 - acc: 0.9561 - val_loss: 0.0976 - val_acc: 0.9609\n",
      "Train on 1240815 samples, validate on 65307 samples\n",
      "Epoch 1/1\n",
      "1240815/1240815 [==============================] - 143s 115us/step - loss: 0.0922 - acc: 0.9629 - val_loss: 0.0953 - val_acc: 0.9614\n"
     ]
    }
   ],
   "source": [
    "pred_val_y, pred_test_y = train_pred(model, epochs = 2) # GloVe only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score at threshold 0.1 is 0.5978407557354926\n",
      "F1 score at threshold 0.11 is 0.606505047019239\n",
      "F1 score at threshold 0.12 is 0.6145704709825366\n",
      "F1 score at threshold 0.13 is 0.6221463239259392\n",
      "F1 score at threshold 0.14 is 0.6281218552739913\n",
      "F1 score at threshold 0.15 is 0.6321166527352094\n",
      "F1 score at threshold 0.16 is 0.6358926048045219\n",
      "F1 score at threshold 0.17 is 0.6396714735937352\n",
      "F1 score at threshold 0.18 is 0.6437137330754352\n",
      "F1 score at threshold 0.19 is 0.6479701902333792\n",
      "F1 score at threshold 0.2 is 0.6511027220345719\n",
      "F1 score at threshold 0.21 is 0.6539350688511408\n",
      "F1 score at threshold 0.22 is 0.6565133516093006\n",
      "F1 score at threshold 0.23 is 0.6606446314925067\n",
      "F1 score at threshold 0.24 is 0.6620375168411234\n",
      "F1 score at threshold 0.25 is 0.6644295302013423\n",
      "F1 score at threshold 0.26 is 0.6647588765235823\n",
      "F1 score at threshold 0.27 is 0.6671664167916043\n",
      "F1 score at threshold 0.28 is 0.6702656013819909\n",
      "F1 score at threshold 0.29 is 0.6716092455298736\n",
      "F1 score at threshold 0.3 is 0.6727633318642574\n",
      "F1 score at threshold 0.31 is 0.6733377807427174\n",
      "F1 score at threshold 0.32 is 0.674069089277703\n",
      "F1 score at threshold 0.33 is 0.6749660479855139\n",
      "F1 score at threshold 0.34 is 0.675943880460819\n",
      "F1 score at threshold 0.35 is 0.6763251695987121\n",
      "F1 score at threshold 0.36 is 0.6761772210623985\n",
      "F1 score at threshold 0.37 is 0.6788261428738455\n",
      "F1 score at threshold 0.38 is 0.6785335376635625\n",
      "F1 score at threshold 0.39 is 0.67753062923754\n",
      "F1 score at threshold 0.4 is 0.6785028790786949\n",
      "F1 score at threshold 0.41 is 0.6797101449275362\n",
      "F1 score at threshold 0.42 is 0.6798005594065426\n",
      "F1 score at threshold 0.43 is 0.6793285136625413\n",
      "F1 score at threshold 0.44 is 0.6794128530899223\n",
      "F1 score at threshold 0.45 is 0.6780336029869322\n",
      "F1 score at threshold 0.46 is 0.6750566322678077\n",
      "F1 score at threshold 0.47 is 0.6736788746673424\n",
      "F1 score at threshold 0.48 is 0.6734850421886985\n",
      "F1 score at threshold 0.49 is 0.6737954135532079\n",
      "F1 score at threshold 0.5 is 0.672474320634508\n",
      "Best threshold:  0.42\n"
     ]
    }
   ],
   "source": [
    "thresholds = []\n",
    "for thresh in np.arange(0.1, 0.501, 0.01):\n",
    "    thresh = np.round(thresh, 2)\n",
    "    res = metrics.f1_score(val_y, (pred_val_y > thresh).astype(int))\n",
    "    thresholds.append([thresh, res])\n",
    "    print(\"F1 score at threshold {0} is {1}\".format(thresh, res))\n",
    "    \n",
    "thresholds.sort(key=lambda x: x[1], reverse=True)\n",
    "best_thresh = thresholds[0][0]\n",
    "print(\"Best threshold: \", best_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_y = (pred_test_y > best_thresh).astype(int)\n",
    "test_df = pd.read_csv(\"../input/test.csv\", usecols=[\"qid\"])\n",
    "out_df = pd.DataFrame({\"qid\":test_df[\"qid\"].values})\n",
    "out_df['prediction'] = pred_test_y\n",
    "out_df.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
